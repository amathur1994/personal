{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7bb9af3-e8f3-4fc0-b0e0-e7e2fa2ce38a",
   "metadata": {},
   "source": [
    "# US Vehicles Pricing Regression \n",
    "\n",
    "Using vehicle features such as engine, fuel, mileage to predict the selling price, using the target variable MSRP.\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/CooperUnion/cardataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16feb770-e6b5-4ba4-b103-c0567af36162",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e2331-745b-4b34-a316-8fc7efb45368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603d825-7560-4751-b7b9-5b2037eb8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the csv file \n",
    "inpath = r'/Users/akhilmathur/Desktop/python_files/datasets/car_data.csv'\n",
    "df = pd.read_csv(inpath)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e9d37-836b-4d44-9d32-135ccb6b9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target and feature columns \n",
    "trgt_col = 'MSRP'\n",
    "ftr_cols = df.columns.to_list()[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73640986-321a-4736-9984-7f1806dac55b",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0239f2-ec4d-4469-8eea-a271c366d371",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772cc73-802d-451a-b250-0923b3567418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49a3bb-e9f7-48d5-9d18-6d43a59e124b",
   "metadata": {},
   "source": [
    "#### Data Cleaning - Missing and Duplicate Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86df0dd-d274-48bd-ade4-3b7bf5fcd7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows in data\n",
    "n_dup = df.duplicated().sum()\n",
    "print(\"{} duplicate rows to be dropped.\".format(n_dup))\n",
    "df.drop_duplicates(keep = 'first', inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74e29f-6967-479a-81b9-0cb14020f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count missing values in each column\n",
    "print(df.isna().sum())\n",
    "\n",
    "# use ffill to impute missing values - as similar car types present together \n",
    "df.fillna(method = 'ffill', inplace = True)\n",
    "print(\"\\nMissing values post-imputation : {}\".format(df.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e292d-6bd2-4b33-a562-8bf95524c33a",
   "metadata": {},
   "source": [
    "#### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121e9e5-ff4a-4177-994e-eabce3433da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for correlations \n",
    "df_num = df.select_dtypes(exclude = 'object')\n",
    "num_cols = df_num.columns.to_list()\n",
    "corr_ = abs(df_num.corr())\n",
    "\n",
    "# generate heatmap\n",
    "sns.heatmap(corr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39d00e-b2bb-473e-8b25-3c6c74d52d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation of numeric features with target variable (MSRP) \n",
    "print(corr_.iloc[:,-1].sort_values(ascending = False))\n",
    "\n",
    "# we remove the redundant features like popularity that have minimal impact on the MSRP target variable \n",
    "df_num.drop('Popularity', axis = 'columns', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd2fc6-b5cd-488f-bc7d-5dcf6ceb1871",
   "metadata": {},
   "source": [
    "#### Numerical Features Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb0e66-18b8-425d-92a8-c2b11411f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the skew - based on this and the above pairplots its clear that the features are not normally distributed \n",
    "print(df_num.skew().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e527063-10dc-4fc2-9457-ed49cf135a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_trsfd = df_num.copy()\n",
    "exc_frm_trns = [trgt_col,'Year','Number of Doors']\n",
    "\n",
    "# apply log transformation to features to reduce skew in distribution \n",
    "for col in df_num.columns:\n",
    "    if col not in exc_frm_trns:\n",
    "        df_num_trsfd[col] = np.log1p(df_num_trsfd[col])\n",
    "\n",
    "# we notice the features becoming slightly less skewed - target variable was left untouched \n",
    "print(df_num_trsfd.skew().sort_values(ascending = False))\n",
    "df_num_trsfd.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65253b-e4ce-4f79-bbcd-e7206bffeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pairplot to observe features relationship with target variable (MSRP) \n",
    "sns.pairplot(df_num_trsfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72cf9fd-4d88-4fa6-9a77-367676abcdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear relationship of features visible with MSRP in above pairplot - add polynomial features \n",
    "columns_to_poly = ['Engine HP','Engine Cylinders','highway MPG', 'city mpg']\n",
    "deg = 3\n",
    "poly = PolynomialFeatures(deg, include_bias=False)\n",
    "X_poly = poly.fit_transform(df_num_trsfd[columns_to_poly])\n",
    "\n",
    "# Combine polynomial features with original DataFrame\n",
    "df_poly = pd.concat([df_num_trsfd, pd.DataFrame(X_poly, columns=poly.get_feature_names_out(columns_to_poly))], axis=1)\n",
    "\n",
    "# save the clean numerical df in df_num\n",
    "df_num_fin = df_poly.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02bc49-4463-450e-9904-8881d55bc7a2",
   "metadata": {},
   "source": [
    "#### Categorical Features Cleanup and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c19809-e16b-4766-8702-387034fc146b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get count of unique values for each categorical value \n",
    "df_cat = df.select_dtypes(include = 'object')\n",
    "for col_ in df_cat.columns:\n",
    "    print(\"{}:{}\".format(col_,df_cat[col_].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82136a-69cd-4ae9-a7d7-ff1edbef04c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_ in df_cat.columns:\n",
    "    print(\"{}:\".format(col_))\n",
    "    print(list(df_cat[col_].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a5b28-f852-41d8-b057-e73dd3426ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate the engine fuel feature categories\n",
    "fuel_map = {\"petrol\" : ['premium unleaded (required)', 'regular unleaded', 'premium unleaded (recommended)'],\n",
    "            \"flex_fuel\" : ['flex-fuel (unleaded/E85)', 'flex-fuel (premium unleaded recommended/E85)', 'flex-fuel (premium unleaded required/E85)', 'flex-fuel (unleaded/natural gas)']\n",
    "            }\n",
    "def fuel_lbl_map(row):\n",
    "    for k_,v_ in fuel_map.items():\n",
    "        if row in v_:\n",
    "            row = k_[:]\n",
    "    return row\n",
    "\n",
    "df_cat['Engine Fuel Type'] = df_cat['Engine Fuel Type'].apply(lambda x: fuel_lbl_map(x))\n",
    "\n",
    "\n",
    "# Consolidate vehicle style categories into broader categories \n",
    "vh_sty_map = {\"Hatchback\" : ['4dr Hatchback', '2dr Hatchback'],\n",
    "               \"Minivan\" : ['Passenger Minivan', 'Cargo Minivan'],\n",
    "               \"SUV\" : ['4dr SUV', '2dr SUV', 'Convertible SUV'],\n",
    "               \"Pickup\" :['Crew Cab Pickup', 'Regular Cab Pickup', 'Extended Cab Pickup'],\n",
    "               \"Van\" : ['Cargo Van','Passenger Van']\n",
    "             }\n",
    "\n",
    "def style_lbl_map(row):\n",
    "    for k_,v_ in vh_sty_map.items():\n",
    "        if row in v_:\n",
    "            row = k_[:]\n",
    "    return row\n",
    "    \n",
    "df_cat['Vehicle Style'] = df_cat['Vehicle Style'].apply(lambda x: style_lbl_map(x))\n",
    "\n",
    "\n",
    "# function to replace market categories with broader labels given above \n",
    "mk_cats_lst = ['Crossover', 'Diesel', 'Exotic', 'Factory Tuner', 'Flex Fuel', 'Hatchback', 'Luxury,']\n",
    "def map_market_cat(row):\n",
    "    for cat_ in mk_cats_lst:\n",
    "        if cat_ in row: \n",
    "            row = cat_[:]\n",
    "    return row\n",
    "    \n",
    "# apply the function and do further cleanup \n",
    "df_cat['Market Category'] = df_cat['Market Category'].apply(lambda x: map_market_cat(x))\n",
    "df_cat['Market Category'] = df_cat['Market Category'].str.replace('Luxury,', 'Luxury')\n",
    "df_cat['Market Category'] = df_cat['Market Category'].str.replace('Performance,Hybrid', 'Hybrid')\n",
    "\n",
    "# we remove the model column due its 915 unique values - will add large no. of feature columns post one hot encoding \n",
    "drop_cols = ['Model']\n",
    "df_cat.drop(drop_cols, axis = 'columns' , inplace = True)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0d95d-cb2b-40fc-9e24-9ae3e789b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique categories post-consolidation:\")\n",
    "for col_ in df_cat.columns:\n",
    "    print(\"{}:{}\".format(col_,df_cat[col_].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6adc00-385c-4aaa-b311-982ba5244c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use one hot encoder to encode categorical label\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "ohe = encoder.fit_transform(df_cat)\n",
    "df_ohe = pd.DataFrame(ohe, columns=encoder.get_feature_names_out(df_cat.columns))\n",
    "\n",
    "# concat with the numerical df to get final dataframe \n",
    "df_fin = pd.concat([df_num_fin, df_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b91df-a766-4f29-be0a-581100aaf2e6",
   "metadata": {},
   "source": [
    "### Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d53bf-b70e-44f1-ba89-4230ccc33f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,StackingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d3916-b847-4a4b-8536-678c2f4c052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features and target data \n",
    "X  = df_fin.drop(trgt_col, axis = 'columns')\n",
    "y = df_fin[trgt_col]\n",
    "\n",
    "\n",
    "# split the dataset into training and testing \n",
    "X_train,X_test,y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 123) \n",
    "\n",
    "# print the shape of the split datasets \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# scale the training and testing data \n",
    "sclr = StandardScaler()\n",
    "X_train = sclr.fit_transform(X_train)\n",
    "X_test = sclr.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53075407-50a6-4e93-8ef1-62eea711c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the training data and predict values for the test data \n",
    "y_pred_dt,r2_dt = dict(),dict()\n",
    "\n",
    "# Simple Linear Regression \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred_dt['linear'] = lr.predict(X_test)\n",
    "r2_dt['linear'] = lr.score(X_test, y_test).round(2)\n",
    "\n",
    "\n",
    "# Ridge Regression (with L2 Regularization) \n",
    "alpha_ridge = 1\n",
    "rr = Ridge(alpha = alpha_ridge)\n",
    "rr.fit(X_train, y_train)\n",
    "y_pred_dt['ridge'] = rr.predict(X_test)\n",
    "r2_dt['ridge'] = rr.score(X_test, y_test).round(2)\n",
    "\n",
    "# LASSO Regression (with L1 Regularization)\n",
    "alpha_lass = 5\n",
    "lass = Lasso(alpha = alpha_lass)\n",
    "lass.fit(X_train, y_train)\n",
    "y_pred_dt['lasso'] = lass.predict(X_test)\n",
    "r2_dt['lasso'] = lass.score(X_test, y_test).round(2)\n",
    "\n",
    "# Elastic Net Regularization (Combine L1 + L2) \n",
    "en = ElasticNet(alpha = 1, l1_ratio = 0.5, random_state =123)\n",
    "en.fit(X_train,y_train)\n",
    "y_pred_dt['elastic_net'] = en.predict(X_test)\n",
    "r2_dt['elastic_net'] = en.score(X_test, y_test).round(2)\n",
    "\n",
    "\n",
    "# Also trying ensemble method to assess predictability vs interpretability dilemma (Ensemble methods being more complex)  \n",
    "# will use Random Forest Regressor \n",
    "rf = RandomForestRegressor(n_estimators = 40, max_depth = None, bootstrap = True, random_state=123)\n",
    "rf_pp = make_pipeline(rf)\n",
    "\n",
    "rf_pp.fit(X_train, y_train)\n",
    "y_pred_dt['Random_Forest'] = rf_pp.predict(X_test)\n",
    "r2_dt['Random_Forest'] =r2_score(y_test, y_pred_dt['Random_Forest']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e777a53-c6c5-46dd-810c-7300efd81031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the model performance across different algorithms \n",
    "df_res = pd.DataFrame(columns = ['Model Type', 'RMSE', 'R-squared Value'])\n",
    "\n",
    "for key,value in y_pred_dt.items():\n",
    "    mse = mean_squared_error(y_test,y_pred_dt[key])\n",
    "    rmse = np.round(math.sqrt(mse),2)\n",
    "    df_res.loc[len(df_res)] = [key, rmse, r2_dt[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4716db3-3454-4d36-ad79-95c43f9e8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-fold cross-validation object\n",
    "kf_cv_res = list()\n",
    "k_folds = 4\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for model in [lr,rr, lass,en, rf]:\n",
    "    cross_val_accuracy = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "    kf_cv_res.append(cross_val_accuracy.mean().round(2))\n",
    "\n",
    "# concat k-fold cv results to results df \n",
    "df_res['k_fold_mean_R2'] = kf_cv_res\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceecf1b-c3ca-4cc7-8da4-88408ecb0602",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "\n",
    "We observe that Random Forest gives a high model score of 95% while also reducing the mean squared error the most. Though it presents a challenge of becoming a black-box model i.e. low intepretability of random forest structure, it is significantly more effective in achieving more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c265a-0eca-4699-81af-a5798dfe58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Optional:\n",
    "# # Try to further improve Random Forest performance by hyperparameter tuning \n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "# }\n",
    "\n",
    "# model = RandomForestRegressor() \n",
    "\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best_params = grid_search.best_params_\n",
    "# best_model = grid_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
